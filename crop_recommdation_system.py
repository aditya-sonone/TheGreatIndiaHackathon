# -*- coding: utf-8 -*-
"""Crop_Recommdation System

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PCGwmhbpFGjglteTV5x9NNejXSypL4Bg
"""

# Commented out IPython magic to ensure Python compatibility.
import IPython
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from IPython import get_ipython
import warnings
warnings.filterwarnings("ignore")

data = pd.read_csv("/content/Crop_recommendation.csv")

data.head(5)

data.tail(5)

data.shape

data.columns

data.duplicated().sum()

data.isnull().sum()

data.info()

data.describe

data.nunique()

data['label'].unique()

data['label'].value_counts()

crop_summary=pd.pivot_table(data,index=['label'],aggfunc='mean')

crop_summary

data.columns

import plotly.express as px

fig = px.box(data,y="N", points = "all")
fig.show()

fig = px.box(data,y = "P",points="all")
fig.show()

data.columns

fig = px.box(data,y = "rainfall",points="all")
fig.show()

df_boston = data
df_boston.columns=df_boston.columns
df_boston.head

Q1 = np.percentile(df_boston['rainfall'],25,
                 interpolation='midpoint')

Q3 = np.percentile(df_boston['rainfall'],75,
                 interpolation='midpoint')

IQR = Q3 - Q1

print("OLD SHAPE:", df_boston.shape)

upper = np.where(df_boston['rainfall'] >=(Q3+1.5*IQR))

lower = np.where(df_boston['rainfall'] <=(Q3-1.5*IQR))

df_boston.drop(upper[0],inplace=True)


df_boston.drop(lower[0],inplace=True)

print("New Shape:",df_boston.shape)

data = df_boston

plt.figure(figsize=(15,6))
sns.barplot(y='N',x='label',data=data,palette='hls')
plt.xticks(rotation=90)
plt.show()

#pip install plotly

crop_summary_new = data.copy()

import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

fig1 = px.bar(crop_summary_new,x = 'label',y = 'N')
fig1.show()

fig1 = px.bar(crop_summary_new,x = 'label',y = 'K')
fig1.show()

import random 
from IPython.core.display import update_display



data.corr()

fig,ax=plt.subplots(1,1,figsize=(15,9))
sns.heatmap(data.corr(),annot=True,cmap='Wistia')
ax.set(xlabel='features')
ax.set(ylabel='features')

plt.title('correlation betwwen differenet features',fontsize=15,c='black')
plt.show()

X = data.drop('label',axis=1)
y = data['label']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,
                                               shuffle=True,random_state=0)

import lightgbm as lgb
model = lgb.LGBMClassifier()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_pred,y_test)
print("LightBGM Model accuracy score:",format(accuracy_score(y_test,y_pred)))

import pickle
pickle.dump(model,open("model.pkl","wb"))
# pickle.dump()

import os
os.getcwd()

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,y_pred)

plt.figure(figsize=(15,15))
sns.heatmap(cm,annot=True,fmt=".0f",linewidths=.5,square = True,cmap='Blues');
plt.ylabel('Acyual label');
plt.xlabel('Prediction label');
all_sample_title = 'Confusion Matrix - score:' +str(accuracy_score(y_test,y_pred))
plt.title(all_sample_title,size = 15);
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

from sklearn.tree import DecisionTreeClassifier
Classifier = DecisionTreeClassifier(criterion = 'entropy', random_state= 0)

Classifier.fit(X_train,y_train)

from sklearn.linear_model import LogisticRegression

classifier_lr = LogisticRegression(random_state=0)

classifier_lr.fit(X_train,y_train)

y_pred=classifier_lr.predict(X_test)

from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_pred,y_test)
print('Logistic Regression Model accuracy score: {0:0.4f}',format(accuracy_score(y_test,y_pred)))

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))



X_test[0:1]

result=Classifier.predict(X_test[0:1])

result

y_test[0:1]